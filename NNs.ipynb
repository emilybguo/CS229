{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emilybguo/CS229/blob/main/NNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook holds code for data preprocessing and the various versions of NN models explored."
      ],
      "metadata": {
        "id": "IQ5krdcePsUH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKod3tZVsgmq"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJI01dcNsAHT"
      },
      "source": [
        "In this file, we preprocess the data to prepare it for testing on machine learning algorithms. This includes controlling for variables, removing those we do not want to consider, and splitting catagorical data into one-hot vectors that allow us to include them in algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvkA7T0_yuWe"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import sklearn \n",
        "\n",
        "# For standardizing dataset\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# library for multiclass model metrics\n",
        "import disarray\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " # For splitting of data into train and test set\n",
        "from sklearn.model_selection import train_test_split\n",
        " \n",
        "# For metrics and confusion matrix\n",
        "from sklearn.metrics  import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "\n",
        "# library for nural network feature importance\n",
        "import shap\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeXVryw4x2uO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWspmRvQybwI"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/gdrive/MyDrive/2022-2023/Fall/CS229/CS229 project/cleaned_data_12_09.csv')\n",
        "df = df.drop('loan_amount', axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TAuqYURru54"
      },
      "outputs": [],
      "source": [
        "x = df.drop(columns = ['action_taken']).copy()\n",
        "y = df['action_taken']\n",
        "\n",
        "# first, split data into training and remaining datasets, with training as 80% of \n",
        "# original dataset size\n",
        "x_train, x_rem, y_train, y_rem = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=0)\n",
        "\n",
        "# next, split remaining data into validation and test datasets, with both as 10%\n",
        "# of the original dataset size (50% of the remaining 20%)\n",
        "x_valid, x_test, y_valid, y_test = train_test_split(x_rem, y_rem, test_size=0.5, shuffle=True, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDBebEZrG7sw"
      },
      "outputs": [],
      "source": [
        "d = {}\n",
        "\n",
        "for x in y_train:\n",
        "    d[x] = d.get(x,0) + 1\n",
        "\n",
        "# printing result\n",
        "print(f\"The list frequency of elements in y_train is : {d}\" )\n",
        "\n",
        "e = {}\n",
        "\n",
        "for x in y_valid:\n",
        "    e[x] = e.get(x,0) + 1\n",
        "\n",
        "# printing result\n",
        "print(f\"The list frequency of elements in y_valid is : {e}\" )\n",
        "\n",
        "f = {}\n",
        "\n",
        "for x in y_test:\n",
        "    f[x] = f.get(x,0) + 1\n",
        "\n",
        "# printing result\n",
        "print(f\"The list frequency of elements in y_valid is : {f}\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlKYOfb5EutN"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(x_train)\n",
        "\n",
        "x_train = scaler.transform(x_train)\n",
        "x_valid = scaler.transform(x_valid)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFs7BL2AYe9t"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.to_numpy()\n",
        "y_valid = y_valid.to_numpy()\n",
        "y_test = y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THRaaXJTLan9"
      },
      "outputs": [],
      "source": [
        "def make_categorical(labels):\n",
        "  labels_categorical = np.zeros((len(labels), 3))\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == 2:\n",
        "      labels_categorical[i][0] = 1\n",
        "    if labels[i] == 3:\n",
        "      labels_categorical[i][1] = 1\n",
        "    if labels[i] == 6:\n",
        "      labels_categorical[i][2] = 1\n",
        "  return labels_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def single_predictions(softmax_predictions):\n",
        "  single_vals = []\n",
        "  for i in range(len(softmax_predictions)):\n",
        "    single_vals.append(np.argmax(softmax_predictions[i]))\n",
        "  return single_vals"
      ],
      "metadata": {
        "id": "C8Bt2-iufHEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syX_2Y5UGq8D"
      },
      "outputs": [],
      "source": [
        "y_train_categorical = make_categorical(y_train)\n",
        "y_valid_categorical = make_categorical(y_valid)\n",
        "y_test_categorical = make_categorical(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3DL_O1v6oOW"
      },
      "source": [
        "# Machine learning algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY8Rm28zCHZk"
      },
      "source": [
        "## Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "justUz_zD-MO"
      },
      "outputs": [],
      "source": [
        "nn1 = Sequential()\n",
        "nn1.add(Dense(12, input_shape=(60,), activation='sigmoid'))\n",
        "nn1.add(Dense(8, activation='sigmoid'))\n",
        "nn1.add(Dense(3, activation='softmax'))\n",
        "\n",
        "nn1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLcDThzSPgng"
      },
      "outputs": [],
      "source": [
        "nn1.fit(x_train, y_train_categorical, epochs=60, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDT4SnKZFVSN"
      },
      "outputs": [],
      "source": [
        "print(nn1.evaluate(x_valid, y_valid_categorical))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_valid_categorical)\n",
        "cm = confusion_matrix(single_predictions(y_valid_categorical), single_predictions(nn1.predict(x_valid)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1SWo1s6dhcae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn2 = Sequential()\n",
        "nn2.add(Dense(300, input_shape=(60,), activation='ReLU'))\n",
        "nn2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "nn2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qBdJvz8qib2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn2.fit(x_train, y_train_categorical, epochs=30, batch_size=10)"
      ],
      "metadata": {
        "id": "fh5Dx7dKirQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn2.evaluate(x_valid, y_valid_categorical))"
      ],
      "metadata": {
        "id": "51HGFyfdjYJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_valid_categorical)\n",
        "cm = confusion_matrix(single_predictions(y_valid_categorical), single_predictions(nn2.predict(x_valid)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0zl_Pmzi48H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn3 = Sequential()\n",
        "nn3.add(Dense(20, input_shape=(60,), activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(20, activation='sigmoid'))\n",
        "nn3.add(Dense(3, activation='softmax'))\n",
        "\n",
        "nn3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5_eP7Muin214"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn3.fit(x_train, y_train_categorical, epochs=60, batch_size=10)"
      ],
      "metadata": {
        "id": "k0-fmoIqoWJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn3.evaluate(x_valid, y_valid_categorical))"
      ],
      "metadata": {
        "id": "x5exQ87WohNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_valid_categorical)\n",
        "cm = confusion_matrix(single_predictions(y_valid_categorical), single_predictions(nn3.predict(x_valid)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k14Vb0Odoaar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4mqXu0ebz3Y"
      },
      "source": [
        "## Testing the Models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn1.evaluate(x_test, y_test_categorical))\n",
        "\n",
        "cm = confusion_matrix(single_predictions(y_test_categorical), single_predictions(nn1.predict(x_test)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "plt.title(\"NN Two Small Sigmoid Layers Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "metrics = pd.DataFrame(cm, dtype=int)\n",
        "metrics.da.precision, metrics.da.recall"
      ],
      "metadata": {
        "id": "Y7H-4yfyGOqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn2.evaluate(x_test, y_test_categorical))\n",
        "\n",
        "cm = confusion_matrix(single_predictions(y_test_categorical), single_predictions(nn2.predict(x_test)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "plt.title(\"NN One Large ReLU Layer Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "metrics = pd.DataFrame(cm, dtype=int)\n",
        "metrics.da.precision, metrics.da.recall"
      ],
      "metadata": {
        "id": "xJZqS8QiQs-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn3.evaluate(x_test, y_test_categorical))\n",
        "\n",
        "cm = confusion_matrix(single_predictions(y_test_categorical), single_predictions(nn3.predict(x_test)), labels=[0, 1, 2])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=[2, 3, 6])\n",
        "disp.plot()\n",
        "plt.title(\"NN Thirteen Sigmoid Layers Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "metrics = pd.DataFrame(cm, dtype=int)\n",
        "metrics.da.precision, metrics.da.recall"
      ],
      "metadata": {
        "id": "4W18_FwLQtGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}